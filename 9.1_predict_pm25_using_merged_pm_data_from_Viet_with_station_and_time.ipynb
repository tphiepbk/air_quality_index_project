{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Og_hH0IXLLvf"
   },
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teCN6WLfxWPP"
   },
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1762076146321,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "wC3MZumn8cfi"
   },
   "outputs": [],
   "source": [
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ndizOjuyOSj"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 30685,
     "status": "error",
     "timestamp": 1762076177008,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "y8WVhe0QM7sK",
    "outputId": "f6e517a8-9c58-4d9b-fd55-56d448cdb196"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762136767.950593     280 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762136767.977862     280 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1762136768.257738     280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762136768.257785     280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762136768.257787     280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1762136768.257789     280 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU to train\n",
      "Tensorflow version: 2.19.1, GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# System\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Data processing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import joblib\n",
    "\n",
    "# Model\n",
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, root_mean_squared_error, mean_absolute_percentage_error\n",
    "from scipy.stats import pearsonr\n",
    "from keras import Input, Model, Sequential\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Dropout, GRU, Conv1D, MaxPooling1D, Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.saving import load_model\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import MeanAbsoluteError\n",
    "from keras.losses import MeanAbsoluteError, MeanSquaredError\n",
    "import keras.backend as K\n",
    "from tqdm.keras import TqdmCallback\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# Append the custom libraries to system path\n",
    "sys.path.append(\"/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project\")\n",
    "\n",
    "# Configure device\n",
    "if not USE_GPU:\n",
    "    print(\"Using CPU to train\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "else:\n",
    "    print(\"Using GPU to train\")\n",
    "\n",
    "# Custom libraries written by myself\n",
    "from src.plot import plot_1_data, plot_2_data, plot_3_data, plot_prediction, plot_learning_curves\n",
    "from src.reduction_model.lstm_s2s import LSTMSeq2SeqReduction\n",
    "from src.reduction_model.gru_s2s import GRUSeq2SeqReduction\n",
    "from src.reduction_model.cnnlstm_s2s import CNNLSTMSeq2SeqReduction\n",
    "from src.prediction_model.lstm import LSTMPrediction, inferenceLSTM\n",
    "from src.loop_model import generate_loopresults, choose_the_best\n",
    "from src.reduce_data_utils import prepareReducedData\n",
    "from src.data_utils import mice\n",
    "from src.time_series_utils import splitTrainValidationTestTimeSeries, reframePastFuture, padPastFuture\n",
    "\n",
    "# Configuration reader\n",
    "from src.config_reader import ConfigurationReader\n",
    "\n",
    "# Checking Tensorflow\n",
    "print(f\"Tensorflow version: {tf.__version__}, GPUs: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYgsQjDursC2"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1762076177010,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "sWzwUFlyroXm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'aod2022': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/MatchingData2022.xlsx',\n",
       "   'target_start_date': '2022-01-01',\n",
       "   'target_end_date': '2022-12-31'},\n",
       "  'aod2021': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/aod_data_daily.csv',\n",
       "   'target_start_date': '2021-01-01',\n",
       "   'target_end_date': '2021-12-31'},\n",
       "  'mpair': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/MPair.csv',\n",
       "   'target_start_date': '2021-01-01',\n",
       "   'target_end_date': '2022-12-31',\n",
       "   'station_2022_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/station2022.csv',\n",
       "   'station_2018_2021_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/station2018-2021.csv',\n",
       "   'merged_data_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/mpair_merged.csv',\n",
       "   'merged_data_dir_all_locations': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/mpair_merged_all_locations.csv',\n",
       "   'merged_data_dir_all_locations_2018_2022': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/mpair_merged_all_locations_2018_2022.csv'},\n",
       "  'cmaq': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/concentration_station'},\n",
       "  'pm2022': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/quan-trac/2022'},\n",
       "  'pm2021': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/quan-trac/2021'},\n",
       "  'cmaq_with_no': {'file_dir': '/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/joined_left_CMAQ.csv'}},\n",
       " 'workspace': {'saved_model_plot_dir': '/le_thanh_van_118/workspace/hiep_workspace/saved_model_plot',\n",
       "  'saved_model_weight_dir': '/le_thanh_van_118/workspace/hiep_workspace/saved_model_weight',\n",
       "  'saved_data_scaler_dir': '/le_thanh_van_118/workspace/hiep_workspace/saved_data_scaler',\n",
       "  'best_model_weight_dir': '/le_thanh_van_118/workspace/hiep_workspace/best_model_weight',\n",
       "  'best_reduced_data_dir': '/le_thanh_van_118/workspace/hiep_workspace/best_reduced_data',\n",
       "  'data_by_location_dir': '/le_thanh_van_118/workspace/hiep_workspace/data_by_location',\n",
       "  'data_by_station_dir': '/le_thanh_van_118/workspace/hiep_workspace/data_by_station',\n",
       "  'data_statistic_dir': '/le_thanh_van_118/workspace/hiep_workspace/data_statistic',\n",
       "  'data_to_store_in_hdfs_dir': '/le_thanh_van_118/workspace/hiep_workspace/data_to_store_in_hdfs'},\n",
       " 'prediction_aod': {'n_past': 7,\n",
       "  'n_future': 1,\n",
       "  'epochs': 200,\n",
       "  'batch_size': 128},\n",
       " 'reduction_aod': {'n_past': 7,\n",
       "  'n_future': 7,\n",
       "  'epochs': 200,\n",
       "  'batch_size': 128,\n",
       "  'min_number_of_features': 10},\n",
       " 'prediction_cmaq': {'n_past': 168,\n",
       "  'n_future': 1,\n",
       "  'epochs': 5,\n",
       "  'batch_size': 32},\n",
       " 'reduction_cmaq': {'n_past': 168,\n",
       "  'n_future': 168,\n",
       "  'epochs': 5,\n",
       "  'batch_size': 32,\n",
       "  'min_number_of_features': 2}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = ConfigurationReader(\"/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/model_params.json\").data\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zy3vTH99Sf87"
   },
   "source": [
    "# DATA FROM VIET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aty5zVlYSf87"
   },
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 46170,
     "status": "aborted",
     "timestamp": 1762076177011,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "pFsH-lSdSf88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/211.csv\n",
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/212.csv\n",
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/213.csv\n",
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/214.csv\n",
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/215.csv\n",
      "Working on /le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station/216.csv\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'sit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sit'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#cols_to_drop = list(filter(lambda col: \"vientham\" in col or \"cmaq\" in col, df_raw.columns)) + [\"unnamed: 0\"]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m#print(f\"Columns to drop = {cols_to_drop}\")\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#df_raw = df_raw.drop(columns=cols_to_drop)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m df_raw\u001b[38;5;241m.\u001b[39mloc[:, features_to_use]\n\u001b[0;32m---> 25\u001b[0m \u001b[43mdf_raw\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m df_raw\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sit'"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/le_thanh_van_118/workspace/hiep_workspace/air_quality_index_project/dataset/merged-data/by-station\"\n",
    "\n",
    "df_raw = []\n",
    "\n",
    "for csv_file in sorted(glob.glob(f\"{dataset_path}/*\")):\n",
    "    print(f\"Working on {csv_file}\")\n",
    "    df_station = pd.read_csv(csv_file)\n",
    "    df_station[\"station\"] = int(os.path.splitext(os.path.basename(csv_file))[0])\n",
    "    df_raw.append(df_station)\n",
    "df_raw = pd.concat(df_raw, axis=0)\n",
    "\n",
    "# Lowercase columns name\n",
    "df_raw.columns = df_raw.columns.map(lambda col: col.lower().strip())\n",
    "\n",
    "# Set time index\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"])\n",
    "df_raw.set_index(\"date\", inplace=True)\n",
    "\n",
    "# Drop columns\n",
    "features_to_use = [\"pm25_quantrac\", \"tsp_quantrac\", \"station\"]\n",
    "#cols_to_drop = list(filter(lambda col: \"vientham\" in col or \"cmaq\" in col, df_raw.columns)) + [\"unnamed: 0\"]\n",
    "#print(f\"Columns to drop = {cols_to_drop}\")\n",
    "#df_raw = df_raw.drop(columns=cols_to_drop)\n",
    "df_raw = df_raw.loc[:, features_to_use]\n",
    "\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46171,
     "status": "aborted",
     "timestamp": 1762076177012,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "9770G7iP6yxf"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TANavZS8vTzE"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xS7J6ADr2D09"
   },
   "source": [
    "### Drop missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46169,
     "status": "aborted",
     "timestamp": 1762076177013,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "F1iLWhL7b1Fs"
   },
   "outputs": [],
   "source": [
    "df = []\n",
    "for station in sorted(df_raw[\"station\"].unique()):\n",
    "    df_station = df_raw[df_raw[\"station\"] == station]\n",
    "    df.append(df_station.dropna())\n",
    "df = pd.concat(df, axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohQJQ9-qtETZ"
   },
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46168,
     "status": "aborted",
     "timestamp": 1762076177014,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "m-xBvYhMtETc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, [\"tsp_quantrac\"]]\n",
    "y = df.loc[:, [\"pm25_quantrac\"]]\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_scaled = feature_scaler.fit_transform(X)\n",
    "print(X_scaled)\n",
    "\n",
    "label_scaler = MinMaxScaler()\n",
    "y_scaled = label_scaler.fit_transform(y)\n",
    "print(y_scaled)\n",
    "\n",
    "df_scaled = pd.DataFrame(np.concatenate((X_scaled, y_scaled), axis=1), index=df.index)\n",
    "df_scaled[\"station\"] = df.loc[:, [\"station\"]]\n",
    "df_scaled.columns = list(X.columns) + list(y.columns) + [\"station\"]\n",
    "\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmXrPiJIHfRu"
   },
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 0,
     "status": "aborted",
     "timestamp": 1762076177015,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "u7VbLf7QHeuC"
   },
   "outputs": [],
   "source": [
    "df_scaled = df_scaled.drop(columns=[\"station\"], errors=\"ignore\")\n",
    "\n",
    "splitter = round(0.8 * len(df_scaled))\n",
    "train_data = df_scaled.iloc[:splitter]\n",
    "test_data = df_scaled.iloc[splitter:]\n",
    "\n",
    "display(train_data)\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBggBVGFtk9M"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6_62rxSePR7"
   },
   "source": [
    "### Model paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "aborted",
     "timestamp": 1762076177019,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "h2AWkZPhSf8_"
   },
   "outputs": [],
   "source": [
    "n_past = 168\n",
    "n_future = 4\n",
    "n_features = 2\n",
    "n_label = 1\n",
    "latent_dim = 32\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1762076177023,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "6Qzd7TBcSf8_"
   },
   "outputs": [],
   "source": [
    "# Calculate MNBE\n",
    "def mean_normalized_bias_error(y_pred, y_actual):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_actual = np.array(y_actual)\n",
    "    return np.mean((y_pred - y_actual) / np.mean(y_actual)) * 100\n",
    "\n",
    "metrics_calculators = {\n",
    "    \"mae\": mean_absolute_error,\n",
    "    \"mse\": mean_squared_error,\n",
    "    \"rmse\": root_mean_squared_error,\n",
    "    \"r2\": r2_score,\n",
    "    \"mape\": mean_absolute_percentage_error,\n",
    "    \"mnbe\": mean_normalized_bias_error,\n",
    "    \"r_coeff\": lambda a,b: pearsonr(a, b)[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfoJHexsSf8_"
   },
   "source": [
    "### Reframe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46172,
     "status": "aborted",
     "timestamp": 1762076177024,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "TUCzm4dC_PG2"
   },
   "outputs": [],
   "source": [
    "# Function to reframe data\n",
    "def reframe(data, n_past, n_future, n_shifted=0):\n",
    "    m = np.arange(0, len(data) - n_past - n_future - n_shifted + 1)\n",
    "    X = np.array([data.iloc[i:(i + n_past), :] for i in m])\n",
    "    y = np.expand_dims(np.array([data.iloc[(i + n_shifted + n_past):(i + n_shifted + n_past + n_future), -1] for i in m]), -1)\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = reframe(train_data, n_past, n_future)\n",
    "X_test, y_test = reframe(test_data, n_past, n_future)\n",
    "\n",
    "X_train_shifted_1, y_train_shifted_1 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_1, y_test_shifted_1 = reframe(test_data, n_past, n_future, 1)\n",
    "print(X_train_shifted_1.shape, y_train_shifted_1.shape, X_test_shifted_1.shape, y_test_shifted_1.shape)\n",
    "\n",
    "X_train_shifted_2, y_train_shifted_2 = reframe(train_data, n_past, n_future, 2)\n",
    "X_test_shifted_2, y_test_shifted_2 = reframe(test_data, n_past, n_future, 2)\n",
    "print(X_train_shifted_2.shape, y_train_shifted_2.shape, X_test_shifted_2.shape, y_test_shifted_2.shape)\n",
    "\n",
    "X_train_shifted_3, y_train_shifted_3 = reframe(train_data, n_past, n_future, 3)\n",
    "X_test_shifted_3, y_test_shifted_3 = reframe(test_data, n_past, n_future, 3)\n",
    "print(X_train_shifted_3.shape, y_train_shifted_3.shape, X_test_shifted_3.shape, y_test_shifted_3.shape)\n",
    "\n",
    "X_train_shifted_4, y_train_shifted_4 = reframe(train_data, n_past, n_future, 4)\n",
    "X_test_shifted_4, y_test_shifted_4 = reframe(test_data, n_past, n_future, 4)\n",
    "print(X_train_shifted_4.shape, y_train_shifted_4.shape, X_test_shifted_4.shape, y_test_shifted_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeV0ac2r0DCK"
   },
   "source": [
    "### LSTM-Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46175,
     "status": "aborted",
     "timestamp": 1762076177027,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "at5CF6yylGKx"
   },
   "outputs": [],
   "source": [
    "def define_lstms2s_model(n_past, n_future, n_features, n_label, name=\"model\"):\n",
    "    # Encoder layers\n",
    "    encoder_inputs = Input(shape=(n_past, n_features))\n",
    "    encoder_lstm_1 = LSTM(128, return_sequences=True, activation=\"relu\")(encoder_inputs)\n",
    "    encoder_lstm_2, state_h, state_c = LSTM(64, return_state=True, activation=\"relu\", dropout=0.2)(encoder_lstm_1)\n",
    "    encoder_dense = Dense(latent_dim)(encoder_lstm_2)\n",
    "    # Repeat layer\n",
    "    decoder_repeat_vector = RepeatVector(n_future)(encoder_dense)\n",
    "    # Decoder layers\n",
    "    decoder_lstm_1 = LSTM(64, return_sequences=True, activation=\"relu\")(decoder_repeat_vector, initial_state=[state_h, state_c])\n",
    "    decoder_lstm_2 = LSTM(128, return_sequences=True, activation=\"relu\", dropout=0.2)(decoder_lstm_1)\n",
    "    decoder_outputs = TimeDistributed(Dense(n_label))(decoder_lstm_2)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    model.name = name\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "    display(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEkEgZjOltxq"
   },
   "source": [
    "#### No shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46192,
     "status": "aborted",
     "timestamp": 1762076177047,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "b5bX_hJ2YuN0"
   },
   "outputs": [],
   "source": [
    "lstms2s = define_lstms2s_model(n_past, n_future, n_features, n_label, \"lstms2s_no_shift\")\n",
    "\n",
    "history = lstms2s.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred = lstms2s.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGuwwUFshFY"
   },
   "source": [
    "#### Shifted 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "aborted",
     "timestamp": 1762076177169,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "ezKGwluXKleg"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_1, y_train_shifted_1 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_1, y_test_shifted_1 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "lstms2s_shifted1 = define_lstms2s_model(n_past, n_future, n_features, n_label, \"lstms2s_shifted_1\")\n",
    "\n",
    "history = lstms2s_shifted1.fit(X_train_shifted_1, y_train_shifted_1,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_1 = lstms2s_shifted1.predict(X_test_shifted_1)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_1[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_1[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DU1EmCT7uxL0"
   },
   "source": [
    "#### Shifted 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "aborted",
     "timestamp": 1762076177170,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "EUAnQ-1zuxL1"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_2, y_train_shifted_2 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_2, y_test_shifted_2 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "lstms2s_shifted_2 = define_lstms2s_model(n_past, n_future, n_features, n_label, \"lstms2s_shifted_2\")\n",
    "\n",
    "history = lstms2s_shifted_2.fit(X_train_shifted_2, y_train_shifted_2,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_2 = lstms2s_shifted_2.predict(X_test_shifted_2)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_2[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_2[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsrfPzvOu6uc"
   },
   "source": [
    "#### Shifted 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 90,
     "status": "aborted",
     "timestamp": 1762076177171,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "7F-WFGl4u6uc"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_3, y_train_shifted_3 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_3, y_test_shifted_3 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "lstms2s_shifted_3 = define_lstms2s_model(n_past, n_future, n_features, n_label, \"lstms2s_shifted_3\")\n",
    "\n",
    "history = lstms2s_shifted_3.fit(X_train_shifted_3, y_train_shifted_3,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_3 = lstms2s_shifted_3.predict(X_test_shifted_3)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_3[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_3[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aq6H7EH9lyAF"
   },
   "source": [
    "#### Shifted 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 51,
     "status": "aborted",
     "timestamp": 1762076177179,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "WXgkD54hlkak"
   },
   "outputs": [],
   "source": [
    "lstms2s_shifted4 = define_lstms2s_model(n_past, n_future, n_features, n_label, \"lstms2s_shifted_4\")\n",
    "\n",
    "history = lstms2s_shifted4.fit(X_train_shifted_4, y_train_shifted_4,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_4 = lstms2s_shifted4.predict(X_test_shifted_4)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_4[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_4[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u85dTWWvTzR"
   },
   "source": [
    "### GRU-Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1762076177181,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "v34jBDwZ8LvU"
   },
   "outputs": [],
   "source": [
    "def define_grus2s_model(n_past, n_future, n_features, n_label, name=\"model\"):\n",
    "    # encoder layers\n",
    "    encoder_inputs = Input(shape=(n_past, n_features))\n",
    "    encoder_gru_1 = GRU(128, return_sequences=True, activation=\"relu\")(encoder_inputs)\n",
    "    encoder_gru_2, state_h = GRU(64, return_state=True, activation=\"relu\")(encoder_gru_1)\n",
    "    encoder_dropout = Dropout(0.2)(encoder_gru_2)\n",
    "    encoder_dense = Dense(latent_dim)(encoder_dropout)\n",
    "    # Repeat layer\n",
    "    decoder_repeat_vector = RepeatVector(n_future)(encoder_dense)\n",
    "    # Decoder layers\n",
    "    decoder_gru_1 = GRU(64, return_sequences=True, activation=\"relu\")(decoder_repeat_vector, initial_state=state_h)\n",
    "    decoder_gru_2 = GRU(128, return_sequences=True, activation=\"relu\")(decoder_gru_1)\n",
    "    decoder_dropout = Dropout(0.2)(decoder_gru_2)\n",
    "    decoder_outputs = TimeDistributed(Dense(n_label))(decoder_dropout)\n",
    "    # Compile the model\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    model.name = name\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "    display(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7702oPz8G5r"
   },
   "source": [
    "#### No shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 52,
     "status": "aborted",
     "timestamp": 1762076177181,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "k9PhDqopYuN0"
   },
   "outputs": [],
   "source": [
    "grus2s = define_grus2s_model(n_past, n_future, n_features, n_label, \"grus2s_no_shift\")\n",
    "\n",
    "history = grus2s.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred = grus2s.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puiOlocu8ZLI"
   },
   "source": [
    "#### Shifted 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 53,
     "status": "aborted",
     "timestamp": 1762076177182,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "x8MUvlGM8ZLJ"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_1, y_train_shifted_1 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_1, y_test_shifted_1 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "grus2s_shifted1 = define_grus2s_model(n_past, n_future, n_features, n_label, \"grus2s_shifted_1\")\n",
    "\n",
    "history = grus2s_shifted1.fit(X_train_shifted_1, y_train_shifted_1,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_1 = grus2s_shifted1.predict(X_test_shifted_1)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_1[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_1[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ujBnqWs8ZLJ"
   },
   "source": [
    "#### Shifted 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "aborted",
     "timestamp": 1762076177208,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "fdrIhDHD8ZLJ"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_2, y_train_shifted_2 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_2, y_test_shifted_2 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "grus2s_shifted_2 = define_grus2s_model(n_past, n_future, n_features, n_label, \"grus2s_shifted_2\")\n",
    "\n",
    "history = grus2s_shifted_2.fit(X_train_shifted_2, y_train_shifted_2,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_2 = grus2s_shifted_2.predict(X_test_shifted_2)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_2[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_2[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFEieT2o8ZLJ"
   },
   "source": [
    "#### Shifted 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "aborted",
     "timestamp": 1762076177209,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "PBsZ6Q7h8ZLJ"
   },
   "outputs": [],
   "source": [
    "X_train_shifted_3, y_train_shifted_3 = reframe(train_data, n_past, n_future, 1)\n",
    "X_test_shifted_3, y_test_shifted_3 = reframe(test_data, n_past, n_future, 1)\n",
    "\n",
    "grus2s_shifted_3 = define_grus2s_model(n_past, n_future, n_features, n_label, \"grus2s_shifted_3\")\n",
    "\n",
    "history = grus2s_shifted_3.fit(X_train_shifted_3, y_train_shifted_3,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_3 = grus2s_shifted_3.predict(X_test_shifted_3)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_3[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_3[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuXPqYYE8ZLJ"
   },
   "source": [
    "#### Shifted 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1762076177209,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "BfZeKXH98ZLJ"
   },
   "outputs": [],
   "source": [
    "grus2s_shifted4 = define_grus2s_model(n_past, n_future, n_features, n_label, \"grus2s_shifted_4\")\n",
    "\n",
    "history = grus2s_shifted4.fit(X_train_shifted_4, y_train_shifted_4,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred_shifted_4 = grus2s_shifted4.predict(X_test_shifted_4)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test_shifted_4[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred_shifted_4[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmjLm3SCtTs8"
   },
   "source": [
    "### CNN-LSTM Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46327,
     "status": "aborted",
     "timestamp": 1762076177210,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "Kh00c0om_z6R"
   },
   "outputs": [],
   "source": [
    "# Encoder layers\n",
    "encoder_inputs = Input(shape=(n_past, n_features))\n",
    "encoder_cnn_1 = Conv1D(filters=128, kernel_size=2, activation=\"relu\")(encoder_inputs)\n",
    "encoder_max_pooling_1 = MaxPooling1D(pool_size=2)(encoder_cnn_1)\n",
    "encoder_cnn_2 = Conv1D(filters=64, kernel_size=2, activation=\"relu\")(encoder_max_pooling_1)\n",
    "encoder_max_pooling_2 = MaxPooling1D(pool_size=2)(encoder_cnn_2)\n",
    "encoder_lstm, state_h, state_c = LSTM(64, return_state=True, activation=\"relu\")(encoder_max_pooling_2)\n",
    "encoder_dropout = Dropout(0.2)(encoder_lstm)\n",
    "encoder_dense = Dense(latent_dim)(encoder_dropout)\n",
    "# Repeat layer\n",
    "decoder_repeat_vector = RepeatVector(n_future)(encoder_dense)\n",
    "# Decoder layers\n",
    "decoder_lstm_1 = LSTM(64, return_sequences=True, activation=\"relu\")(decoder_repeat_vector, initial_state=[state_h, state_c])\n",
    "decoder_dropout = Dropout(0.2)(decoder_lstm_1)\n",
    "decoder_outputs = TimeDistributed(Dense(n_label))(decoder_dropout)\n",
    "# Compile the model\n",
    "cnncnnlstms2smodel = Model(encoder_inputs, decoder_outputs)\n",
    "# Compile the model\n",
    "cnnlstms2smodel = Model(encoder_inputs, decoder_outputs)\n",
    "cnnlstms2smodel.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n",
    "display(cnnlstms2smodel.summary())\n",
    "\n",
    "# Fit model\n",
    "history = cnnlstms2smodel.fit(X_train, y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                validation_split=0.2,\n",
    "                shuffle=False,\n",
    "                callbacks = [\n",
    "                    #EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True),\n",
    "                    TqdmCallback(verbose=1)\n",
    "                ],\n",
    "                verbose=1)\n",
    "plot_learning_curves(history)\n",
    "\n",
    "# Predict data\n",
    "y_pred = cnnlstms2smodel.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "for day in range(n_future):\n",
    "    print(f\"Day = {day}\")\n",
    "    inv_y_test = label_scaler.inverse_transform(y_test[:, day, :])[:-(day+1)]\n",
    "    inv_y_pred = label_scaler.inverse_transform(y_pred[:, day, :])[(day+1):]\n",
    "    for metric, calculator in metrics_calculators.items():\n",
    "        print(f\"{metric}: {calculator(inv_y_pred, inv_y_test)}\")\n",
    "    plot_2_data(data1=inv_y_test, datalabel1=\"y_test\", data2=inv_y_pred, datalabel2=\"y_predicted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CIILdCxSf9B"
   },
   "source": [
    "## Evaluation board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46326,
     "status": "aborted",
     "timestamp": 1762076177211,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "gqb5VcviSf9B"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(evaluation_board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46345,
     "status": "aborted",
     "timestamp": 1762076177231,
     "user": {
      "displayName": "Phúc Hiệp Thái",
      "userId": "13394645229639472407"
     },
     "user_tz": -420
    },
    "id": "agJnuPArSf9C"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
