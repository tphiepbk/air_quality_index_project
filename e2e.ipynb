{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ffe090-17eb-44d9-b4b3-915e3080661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== Config ====\n",
    "DATA_SOURCE = \"toy\"  # \"toy\" hoặc \"csv\"\n",
    "CSV_PATH = \"...pm25_data.csv\"  # dùng khi DATA_SOURCE=\"csv\"\n",
    "\n",
    "# Nếu dùng CSV thật, chỉnh đúng tên cột (cột đầu tiên trong FEATURES sẽ được coi là PM2.5)\n",
    "FEATURES = [\"PM2.5\", \"NO2\", \"O3\", \"TEMP\", \"RH\"]\n",
    "TARGET_COL = \"PM2.5\"\n",
    "\n",
    "# Cửa sổ & horizon\n",
    "L = 48     # history length\n",
    "H = 6      # số bước dự báo (1 = one-step; >1 = multi-step)\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "# Model & train\n",
    "ENC_HIDDEN = 64\n",
    "DEC_HIDDEN = 64\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.0\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "SEED = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ce81af-7975-43b2-8a10-1d5f9477c000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.6)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Downloading triton-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.0-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:01:39\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:58\u001b[0m6m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m  \u001b[33m0:01:09\u001b[0m6m0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:23\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:29\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:30\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m  \u001b[33m0:00:30\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.5.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0m6m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82━━\u001b[0m \u001b[32m 5/21\u001b[0m [nvidia-nvshmem-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.23.4━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.23.4:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.23.4━━━━━━━━\u001b[0m \u001b[32m 6/21\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82━━━\u001b[0m \u001b[32m 7/21\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82m \u001b[32m 8/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:━━━━━━━━━━━━\u001b[0m \u001b[32m 8/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82[0m \u001b[32m 8/21\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82━\u001b[0m \u001b[32m10/21\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82[0m \u001b[32m11/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82━\u001b[0m \u001b[32m11/21\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu120m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2━━\u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/21\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2━━━━━━━━\u001b[0m \u001b[32m13/21\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m15/21\u001b[0m [fsspec]x]blas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\u001b[0m \u001b[32m15/21\u001b[0m [fsspec]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:[90m━━━━━━━━━━━\u001b[0m \u001b[32m15/21\u001b[0m [fsspec]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3━━\u001b[0m \u001b[32m15/21\u001b[0m [fsspec]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61━━━━\u001b[0m \u001b[32m16/21\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75━━━━━\u001b[0m \u001b[32m17/21\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu120m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83[0m \u001b[32m18/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m18/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83━\u001b[0m \u001b[32m18/21\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [torch]m20/21\u001b[0m [torch]-cusolver-cu12]\n",
      "\u001b[1A\u001b[2KSuccessfully installed fsspec-2025.9.0 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.0 triton-3.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f154317d590>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torch\n",
    "import math, random, numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c8ecc7-b35c-477c-9114-c8c43d9a5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_toy_dataset(T=3000):\n",
    "    t = np.arange(T)\n",
    "    daily  = 10*np.sin(2*np.pi*t/24.0)\n",
    "    weekly =  5*np.sin(2*np.pi*t/(24.0*7))\n",
    "    noise  = np.random.normal(0, 2, size=T)\n",
    "\n",
    "    NO2  = 30 + 5*np.sin(2*np.pi*(t-3)/24.0)  + np.random.normal(0,1,size=T)\n",
    "    O3   = 50 + 7*np.sin(2*np.pi*(t-12)/24.0) + np.random.normal(0,1,size=T)\n",
    "    TEMP = 27 + 4*np.sin(2*np.pi*t/24.0)      + np.random.normal(0,0.3,size=T)\n",
    "    RH   = 70 +10*np.sin(2*np.pi*(t-6)/24.0)  + np.random.normal(0,1,size=T)\n",
    "\n",
    "    PM25 = 40 + 0.6*np.roll(daily,1) + 0.4*np.roll(weekly,24) \\\n",
    "           + 0.15*NO2 - 0.1*np.roll(O3,6) - 0.2*(TEMP-27) + 0.05*(RH-70) + noise\n",
    "    PM25 = np.clip(PM25, 5, None)\n",
    "    data = np.stack([PM25, NO2, O3, TEMP, RH], axis=1)\n",
    "    return pd.DataFrame(data, columns=[\"PM2.5\",\"NO2\",\"O3\",\"TEMP\",\"RH\"])\n",
    "\n",
    "def load_data():\n",
    "    if DATA_SOURCE == \"toy\":\n",
    "        return make_toy_dataset(3000)\n",
    "    else:\n",
    "        df = pd.read_csv(CSV_PATH)\n",
    "        df = df[FEATURES].copy().dropna().reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "df = load_data()\n",
    "display(df.head())\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a9030-e6af-496e-aba5-08353d60ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDS(Dataset):\n",
    "    def __init__(self, X, y, L, H):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.L = L; self.H = H\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.L - self.H + 1\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i:i+self.L]                           # (L, D)\n",
    "        target = self.y[i+self.L:i+self.L+self.H]        # (H,)\n",
    "        return torch.tensor(x), torch.tensor(target).unsqueeze(-1)  # (H,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_all = scaler.fit_transform(df.values)    # (T, D)\n",
    "y_all = df[TARGET_COL].values              # (T,)\n",
    "\n",
    "T_total = len(df)\n",
    "T_val = int(T_total*VAL_RATIO)\n",
    "T_train = T_total - T_val\n",
    "\n",
    "X_train, X_val = X_all[:T_train], X_all[T_train:]\n",
    "y_train, y_val = y_all[:T_train], y_all[T_train:]\n",
    "\n",
    "train_ds = WindowDS(X_train, y_train, L, H)\n",
    "val_ds   = WindowDS(X_val,   y_val,   L, H)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "D = X_all.shape[1]\n",
    "print(f\"D={D}, |train|={len(train_ds)}, |val|={len(val_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705afa1-a597-4f9d-aab4-0ab8aee3b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, enc_hidden, dec_hidden):\n",
    "        super().__init__()\n",
    "        self.Wa = nn.Linear(enc_hidden, dec_hidden, bias=False)\n",
    "\n",
    "    def forward(self, encoder_h, s_prev):\n",
    "        # encoder_h: (B, L, H_enc)\n",
    "        # s_prev:    (B, H_dec)\n",
    "        Wa_h  = self.Wa(encoder_h)                  # (B, L, H_dec)\n",
    "        s_prev = s_prev.unsqueeze(1)                # (B, 1, H_dec)\n",
    "        scores = torch.bmm(Wa_h, s_prev.transpose(1,2)).squeeze(-1)  # (B, L)\n",
    "        attn   = torch.softmax(scores, dim=1)       # (B, L)\n",
    "        context = torch.bmm(attn.unsqueeze(1), encoder_h).squeeze(1) # (B, H_enc)\n",
    "        return context, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec4df0-1b40-4cc4-9f82-aa41fa97697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, input_dim, enc_hidden, dec_hidden, num_layers=1, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, enc_hidden, num_layers=num_layers,\n",
    "                               batch_first=True,\n",
    "                               dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.decoder = nn.LSTMCell(1, dec_hidden)\n",
    "        self.attn    = LuongAttention(enc_hidden, dec_hidden)\n",
    "        self.bridge_h = nn.Linear(enc_hidden, dec_hidden)\n",
    "        self.bridge_c = nn.Linear(enc_hidden, dec_hidden)\n",
    "        self.out = nn.Linear(dec_hidden + enc_hidden, 1)\n",
    "\n",
    "    def forward(self, x, y=None, teacher_forcing_ratio=0.0, horizon=1, return_attn=False):\n",
    "        B = x.size(0)\n",
    "        enc_out, (hT, cT) = self.encoder(x)     # enc_out: (B,L,H_enc)\n",
    "        h = self.bridge_h(hT[-1])               # (B,H_dec)\n",
    "        c = self.bridge_c(cT[-1])               # (B,H_dec)\n",
    "        dec_in = x[:, -1, 0:1]                  # (B,1) giả sử cột 0 là PM2.5\n",
    "\n",
    "        preds = []\n",
    "        attn_list = []\n",
    "        for t in range(horizon):\n",
    "            h, c = self.decoder(dec_in, (h, c))          # h: (B,H_dec)\n",
    "            context, attn = self.attn(enc_out, h)        # context: (B,H_enc), attn: (B,L)\n",
    "            if return_attn: attn_list.append(attn)\n",
    "            cat  = torch.cat([h, context], dim=-1)       # (B,H_dec+H_enc)\n",
    "            pred = self.out(cat).unsqueeze(1)            # (B,1,1)\n",
    "            preds.append(pred)\n",
    "\n",
    "            if (self.training and y is not None and random.random() < teacher_forcing_ratio):\n",
    "                dec_in = y[:, t, :]                       # teacher forcing\n",
    "            else:\n",
    "                dec_in = pred.squeeze(1)\n",
    "\n",
    "        preds = torch.cat(preds, dim=1)                   # (B,H,1)\n",
    "        if return_attn:\n",
    "            return preds, torch.stack(attn_list, dim=1)   # (B,H,L)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc854ca4-aa2f-41b0-b264-939302a12d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = Seq2SeqAttention(D, ENC_HIDDEN, DEC_HIDDEN, NUM_LAYERS, DROPOUT).to(device)\n",
    "opt    = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def run_epoch(loader, train=True, tf_ratio=0.5):\n",
    "    if train: model.train()\n",
    "    else: model.eval()\n",
    "    total=0.0; n=0\n",
    "    with torch.set_grad_enabled(train):\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            if train:\n",
    "                opt.zero_grad()\n",
    "                pred = model(xb, y=yb, teacher_forcing_ratio=tf_ratio, horizon=H)\n",
    "                loss = loss_fn(pred, yb)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                opt.step()\n",
    "            else:\n",
    "                pred = model(xb, y=None, teacher_forcing_ratio=0.0, horizon=H)\n",
    "                loss = loss_fn(pred, yb)\n",
    "            total += loss.item()*xb.size(0); n += xb.size(0)\n",
    "    return total/n\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr = run_epoch(train_loader, True, TEACHER_FORCING_RATIO)\n",
    "    va = run_epoch(val_loader,   False, 0.0)\n",
    "    print(f\"Epoch {epoch:02d} | Train {tr:.4f} | Val {va:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c79d716-7a90-4b32-b7b0-1a0b658d226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "xb, yb = next(iter(val_loader))\n",
    "xb = xb.to(device)\n",
    "with torch.no_grad():\n",
    "    preds, attn = model(xb, horizon=H, return_attn=True)\n",
    "\n",
    "A = attn.detach().cpu().numpy()[0]   # (H, L) — mỗi hàng là 1 bước dự báo\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.imshow(A, aspect='auto')  # không set màu để dùng mặc định\n",
    "plt.title(\"Attention Heatmap (rows=forecast steps, cols=encoder time)\")\n",
    "plt.xlabel(\"Encoder time (1..L)\")\n",
    "plt.ylabel(\"Forecast step (1..H)\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
